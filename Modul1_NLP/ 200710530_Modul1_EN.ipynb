{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inisialisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import gutenberg\n",
    "from pprint import pprint\n",
    "import re\n",
    "import string\n",
    "import collections\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/krisna/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('gutenberg')\n",
    "alice = gutenberg.raw(fileids='carroll-alice.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = 'We will discuss briefly about the basic syntax,\\\n",
    "structure and design philosophies. \\\n",
    "There is a defined hierarchical syntax for Python code which you should␣↪remember \\when writing code! Python is a really powerful programming language!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I. Down the Rabbit-Hole\\n\\nAlice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once or twice she had peeped into the\\nbook her sister was reading, but it had no pictures or conversations in\\nit, 'and what is the use of a book,' thought Alice 'without pictures or\\nconversation?'\\n\\nSo she was considering in her own mind (as well as she could, for the\\nhot day made her feel very sleepy and stupid), whether the pleasure\\nof making a daisy-chain would be worth the trouble of getting up and\\npicking the daisies, when suddenly a White Rabbit with pink eyes ran\\nclose by her.\\n\\nThere was nothing so VERY remarkable in that; nor did Alice think it so\\nVERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\\nOh dear! I shall be late!' (when she thought it over afterwards, it\\noccurred to her that she ought to have wondered at this, but at the time\\nit all seemed quite natural); but\""
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We will discuss briefly about the basic syntax,structure and design philosophies. There is a defined hierarchical syntax for Python code which you should␣↪remember \\\\when writing code! Python is a really powerful programming language!'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144395\n",
      "--------------------------------\n",
      "[Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
      "\n",
      "CHAPTER I. Down the Rabbit-Hole\n",
      "\n",
      "Alice was\n"
     ]
    }
   ],
   "source": [
    "# Total character di Alice in Wonderland(corpora)\n",
    "print (len(alice))\n",
    "print(\"--------------------------------\")\n",
    "# 100 character pertama di corpus\n",
    "print (alice[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisasi Kalimat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/krisna/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## default tokenisasi kalimat\n",
    "nltk.download('punkt')\n",
    "default_st = nltk.sent_tokenize\n",
    "alice_sentences = default_st(text=alice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "\n",
      "Total sentences in alice: 1625\n",
      "First 5 sentences in alice:-\n",
      "[\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I.\",\n",
      " 'Down the Rabbit-Hole\\n'\n",
      " '\\n'\n",
      " 'Alice was beginning to get very tired of sitting by her sister on the\\n'\n",
      " 'bank, and of having nothing to do: once or twice she had peeped into the\\n'\n",
      " 'book her sister was reading, but it had no pictures or conversations in\\n'\n",
      " \"it, 'and what is the use of a book,' thought Alice 'without pictures or\\n\"\n",
      " \"conversation?'\",\n",
      " 'So she was considering in her own mind (as well as she could, for the\\n'\n",
      " 'hot day made her feel very sleepy and stupid), whether the pleasure\\n'\n",
      " 'of making a daisy-chain would be worth the trouble of getting up and\\n'\n",
      " 'picking the daisies, when suddenly a White Rabbit with pink eyes ran\\n'\n",
      " 'close by her.',\n",
      " 'There was nothing so VERY remarkable in that; nor did Alice think it so\\n'\n",
      " \"VERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\",\n",
      " 'Oh dear!']\n"
     ]
    }
   ],
   "source": [
    "print (\"-------------------------------------------\")\n",
    "print ('\\nTotal sentences in alice:', len(alice_sentences))\n",
    "print ('First 5 sentences in alice:-')\n",
    "pprint(alice_sentences[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentences = default_st(text=sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences in sample_text: 3\n",
      "Sample text sentences :-\n",
      "['We will discuss briefly about the basic syntax,structure and design '\n",
      " 'philosophies.',\n",
      " 'There is a defined hierarchical syntax for Python code which you '\n",
      " 'should␣↪remember \\\\when writing code!',\n",
      " 'Python is a really powerful programming language!']\n"
     ]
    }
   ],
   "source": [
    "## print default tokenisasi kalimat\n",
    "print ('Total sentences in sample_text:', len(sample_sentences))\n",
    "print ('Sample text sentences :-')\n",
    "pprint (sample_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We will discuss briefly about the basic syntax,structure and design philosophies.', 'There is a defined hierarchical syntax for Python code which you should␣↪remember \\\\when writing code!', 'Python is a really powerful programming language!']\n"
     ]
    }
   ],
   "source": [
    "print(sample_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisasi Kalimat Bahasa Jerman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import europarl_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157171\n",
      " \n",
      "Wiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package europarl_raw to\n",
      "[nltk_data]     /home/krisna/nltk_data...\n",
      "[nltk_data]   Package europarl_raw is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('europarl_raw')\n",
    "german_text = europarl_raw.german.raw(fileids='ep-00-01-17.de')\n",
    "# Total character di corpus\n",
    "print (len(german_text))\n",
    "# 100 character pertama di corpus\n",
    "print (german_text[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "## default tokenisasi kalimat\n",
    "german_sentences_def = default_st(text=german_text, language='german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memanggil PunktSentenceTokenizer untuk tokenisasi Bahasa Jerman\n",
    "german_tokenizer = nltk.data.load(resource_url='tokenizers/punkt/german.pickle')\n",
    "german_sentences = german_tokenizer.tokenize(german_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.tokenize.punkt.PunktSentenceTokenizer'>\n"
     ]
    }
   ],
   "source": [
    "# Verifikasi type german tokenisasi\n",
    "# harus menjadi PunktSentenceTokenizer\n",
    "print (type(german_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      " \n",
      "Wiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen , wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe , daß Sie schöne Ferien hatten .\n",
      "Wie Sie feststellen konnten , ist der gefürchtete \" Millenium-Bug \" nicht eingetreten .\n",
      "Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden .\n",
      "Im Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nächsten Tagen .\n",
      "Heute möchte ich Sie bitten - das ist auch der Wunsch einiger Kolleginnen und Kollegen - , allen Opfern der Stürme , insbesondere in den verschiedenen Ländern der Europäischen Union , in einer Schweigeminute zu gedenken .\n"
     ]
    }
   ],
   "source": [
    "# cek jika hasil dari tokenisasi sama\n",
    "# harus menjadi True\n",
    "print (german_sentences_def == german_sentences)\n",
    "# print 5 kalimat pertama pada corpus\n",
    "for sent in german_sentences[0:5]:print (sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We will discuss briefly about the basic syntax,structure and design '\n",
      " 'philosophies.',\n",
      " 'There is a defined hierarchical syntax for Python code which you '\n",
      " 'should␣↪remember \\\\when writing code!',\n",
      " 'Python is a really powerful programming language!']\n"
     ]
    }
   ],
   "source": [
    "punkt_st = nltk.tokenize.PunktSentenceTokenizer()\n",
    "sample_sentences = punkt_st.tokenize(sample_text)\n",
    "pprint(sample_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "## menggunakan RegexpTokenizer untuk tokenisasi kalimat\n",
    "SENTENCE_TOKENS_PATTERN = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z]\\.)(?<=\\.|\\?|\\!)\\s'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_st = nltk.tokenize.RegexpTokenizer(pattern=SENTENCE_TOKENS_PATTERN,gaps=True)\n",
    "sample_sentences = regex_st.tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metode Lain untuk Tokenisasi Kalimat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We will discuss briefly about the basic syntax,structure and design '\n",
      " 'philosophies.',\n",
      " 'There is a defined hierarchical syntax for Python code which you '\n",
      " 'should␣↪remember \\\\when writing code!',\n",
      " 'Python is a really powerful programming language!']\n"
     ]
    }
   ],
   "source": [
    "punkt_st = nltk.tokenize.PunktSentenceTokenizer()\n",
    "sample_sentences = punkt_st.tokenize(sample_text)\n",
    "pprint(sample_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We will discuss briefly about the basic syntax,structure and design '\n",
      " 'philosophies.',\n",
      " 'There is a defined hierarchical syntax for Python code which you '\n",
      " 'should␣↪remember \\\\when writing code!',\n",
      " 'Python is a really powerful programming language!']\n"
     ]
    }
   ],
   "source": [
    "SENTENCE_TOKENS_PATTERN = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z]\\.)(?<=\\.|\\?|\\!)\\s'\n",
    "regex_st = nltk.tokenize.RegexpTokenizer(\n",
    "pattern=SENTENCE_TOKENS_PATTERN,\n",
    "gaps=True)\n",
    "sample_sentences = regex_st.tokenize(sample_text)\n",
    "pprint(sample_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisasi Kata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "##contoh kalimat\n",
    "sentence = \"The brown fox wasn't that quick and he couldn't win the race\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'was', \"n't\", 'that', 'quick', 'and', 'he', 'could', \"n't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "default_wt = nltk.word_tokenize\n",
    "words = default_wt(sentence)\n",
    "print (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'was', \"n't\", 'that', 'quick', 'and', 'he', 'could', \"n't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "treebank_wt = nltk.TreebankWordTokenizer()\n",
    "words = treebank_wt.tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'wasn', 't', 'that', 'quick', 'and', 'he', 'couldn', 't', 'win', 'the', 'race']\n",
      "---------------------------------------------\n",
      "['The', 'brown', 'fox', \"wasn't\", 'that', 'quick', 'and', 'he', \"couldn't\", 'win', 'the', 'race']\n",
      "---------------------------------------------\n",
      "[(0, 3), (4, 9), (10, 13), (14, 20), (21, 25), (26, 31), (32, 35), (36, 38), (39, 47), (48, 51), (52, 55), (56, 60)]\n",
      "---------------------------------------------\n",
      "['The', 'brown', 'fox', \"wasn't\", 'that', 'quick', 'and', 'he', \"couldn't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "TOKEN_PATTERN = r'\\w+'\n",
    "regex_wt = nltk.RegexpTokenizer(pattern=TOKEN_PATTERN,\n",
    "gaps=False)\n",
    "words = regex_wt.tokenize(sentence)\n",
    "print (words)\n",
    "print(\"---------------------------------------------\")\n",
    "GAP_PATTERN = r'\\s+'\n",
    "regex_wt = nltk.RegexpTokenizer(pattern=GAP_PATTERN,\n",
    "gaps=True)\n",
    "words = regex_wt.tokenize(sentence)\n",
    "print (words)\n",
    "print(\"---------------------------------------------\")\n",
    "word_indices = list(regex_wt.span_tokenize(sentence))\n",
    "print (word_indices)\n",
    "print(\"---------------------------------------------\")\n",
    "print ([sentence[start:end] for start, end in word_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'wasn', \"'\", 't', 'that', 'quick', 'and', 'he', 'couldn', \"'\", 't', 'win', 'the', 'race']\n",
      "---------------------------------------------\n",
      "['The', 'brown', 'fox', \"wasn't\", 'that', 'quick', 'and', 'he', \"couldn't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "# derived regex tokenisasi\n",
    "wordpunkt_wt = nltk.WordPunctTokenizer()\n",
    "words = wordpunkt_wt.tokenize(sentence)\n",
    "print (words)\n",
    "print(\"---------------------------------------------\")\n",
    "whitespace_wt = nltk.WhitespaceTokenizer()\n",
    "words = whitespace_wt.tokenize(sentence)\n",
    "print (words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from contraction import CONTRACTION_MAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"The brown fox wasn't that quick and he couldn't win the race\",\n",
    "\"Hey that's a great deal! I just bought a phone for $199\",\n",
    "\"@@You'll (learn) a **lot** in the book. Python is an amazing␣↪language!@@\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['The', 'brown', 'fox', 'was', \"n't\", 'that', 'quick', 'and', 'he', 'could', \"n't\", 'win', 'the', 'race']], [['Hey', 'that', \"'s\", 'a', 'great', 'deal', '!'], ['I', 'just', 'bought', 'a', 'phone', 'for', '$', '199']], [['@', '@', 'You', \"'ll\", '(', 'learn', ')', 'a', '*', '*', 'lot', '*', '*', 'in', 'the', 'book', '.'], ['Python', 'is', 'an', 'amazing␣↪language', '!'], ['@', '@']]]\n"
     ]
    }
   ],
   "source": [
    "## tokenisasi bacaan\n",
    "def tokenize_text(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    word_tokens = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "token_list = [tokenize_text(text)\n",
    "for text in corpus]\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_characters_after_tokenization(tokens):\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_tokens = filter(None, [pattern.sub('', token) for token in tokens])\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_list_1 =[filter(None,[remove_characters_after_tokenization(tokens)\n",
    "    for tokens in sentence_tokens])\n",
    "    for sentence_tokens in token_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<filter object at 0x7f3c79ae65c0>, <filter object at 0x7f3c79ae4a90>, <filter object at 0x7f3c79ae72b0>]\n"
     ]
    }
   ],
   "source": [
    "print (filtered_list_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The brown fox wasnt that quick and he couldnt win the race', 'Hey thats a great deal I just bought a phone for 199', 'Youll learn a lot in the book Python is an amazinglanguage']\n",
      "[\"The brown fox wasn't that quick and he couldn't win the race\", \"Hey that's a great deal! I just bought a phone for 199\", \"You'll learn a lot in the book. Python is an amazing␣↪language!\"]\n"
     ]
    }
   ],
   "source": [
    "def remove_characters_before_tokenization(sentence,keep_apostrophes=False):\n",
    "    sentence = sentence.strip()\n",
    "    if keep_apostrophes:\n",
    "        PATTERN = r'[?|$|&|*|%|@|(|)|~]'\n",
    "        filtered_sentence = re.sub(PATTERN, r'', sentence)\n",
    "    else:\n",
    "        PATTERN = r'[^a-zA-Z0-9 ]'\n",
    "        filtered_sentence = re.sub(PATTERN, r'', sentence)\n",
    "    return filtered_sentence\n",
    "filtered_list_2 = [remove_characters_before_tokenization(sentence)\n",
    "for sentence in corpus]\n",
    "print (filtered_list_2)\n",
    "cleaned_corpus = [remove_characters_before_tokenization(sentence,keep_apostrophes=True)\n",
    "for sentence in corpus]\n",
    "print (cleaned_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The brown fox was not that quick and he could not win the race', 'Hey that is a great deal! I just bought a phone for 199', 'You will learn a lot in the book. Python is an amazing␣↪language!']\n",
      "-------------------------------------------------------\n",
      "{\"ain't\": 'is not', \"aren't\": 'are not', \"can't\": 'cannot', \"can't've\": 'cannot have', \"'cause\": 'because', \"could've\": 'could have', \"couldn't\": 'could not', \"couldn't've\": 'could not have', \"didn't\": 'did not', \"doesn't\": 'does not', \"don't\": 'do not', \"hadn't\": 'had not', \"hadn't've\": 'had not have', \"hasn't\": 'has not', \"haven't\": 'have not', \"he'd\": 'he would', \"he'd've\": 'he would have', \"he'll\": 'he will', \"he'll've\": 'he he will have', \"he's\": 'he is', \"how'd\": 'how did', \"how'd'y\": 'how do you', \"how'll\": 'how will', \"how's\": 'how is', \"I'd\": 'I would', \"I'd've\": 'I would have', \"I'll\": 'I will', \"I'll've\": 'I will have', \"I'm\": 'I am', \"I've\": 'I have', \"i'd\": 'i would', \"i'd've\": 'i would have', \"i'll\": 'i will', \"i'll've\": 'i will have', \"i'm\": 'i am', \"i've\": 'i have', \"isn't\": 'is not', \"it'd\": 'it would', \"it'd've\": 'it would have', \"it'll\": 'it will', \"it'll've\": 'it will have', \"it's\": 'it is', \"let's\": 'let us', \"ma'am\": 'madam', \"mayn't\": 'may not', \"might've\": 'might have', \"mightn't\": 'might not', \"mightn't've\": 'might not have', \"must've\": 'must have', \"mustn't\": 'must not', \"mustn't've\": 'must not have', \"needn't\": 'need not', \"needn't've\": 'need not have', \"o'clock\": 'of the clock', \"oughtn't\": 'ought not', \"oughtn't've\": 'ought not have', \"shan't\": 'shall not', \"sha'n't\": 'shall not', \"shan't've\": 'shall not have', \"she'd\": 'she would', \"she'd've\": 'she would have', \"she'll\": 'she will', \"she'll've\": 'she will have', \"she's\": 'she is', \"should've\": 'should have', \"shouldn't\": 'should not', \"shouldn't've\": 'should not have', \"so've\": 'so have', \"so's\": 'so as', \"that'd\": 'that would', \"that'd've\": 'that would have', \"that's\": 'that is', \"there'd\": 'there would', \"there'd've\": 'there would have', \"there's\": 'there is', \"they'd\": 'they would', \"they'd've\": 'they would have', \"they'll\": 'they will', \"they'll've\": 'they will have', \"they're\": 'they are', \"they've\": 'they have', \"to've\": 'to have', \"wasn't\": 'was not', \"we'd\": 'we would', \"we'd've\": 'we would have', \"we'll\": 'we will', \"we'll've\": 'we will have', \"we're\": 'we are', \"we've\": 'we have', \"weren't\": 'were not', \"what'll\": 'what will', \"what'll've\": 'what will have', \"what're\": 'what are', \"what's\": 'what is', \"what've\": 'what have', \"when's\": 'when is', \"when've\": 'when have', \"where'd\": 'where did', \"where's\": 'where is', \"where've\": 'where have', \"who'll\": 'who will', \"who'll've\": 'who will have', \"who's\": 'who is', \"who've\": 'who have', \"why's\": 'why is', \"why've\": 'why have', \"will've\": 'will have', \"won't\": 'will not', \"won't've\": 'will not have', \"would've\": 'would have', \"wouldn't\": 'would not', \"wouldn't've\": 'would not have', \"y'all\": 'you all', \"y'all'd\": 'you all would', \"y'all'd've\": 'you all would have', \"y'all're\": 'you all are', \"y'all've\": 'you all have', \"you'd\": 'you would', \"you'd've\": 'you would have', \"you'll\": 'you will', \"you'll've\": 'you will have', \"you're\": 'you are', \"you've\": 'you have'}\n"
     ]
    }
   ],
   "source": [
    "def expand_contractions(sentence, contraction_mapping):\n",
    "    contractions_pattern = re.compile('({})'.format('|'.\n",
    "    join(contraction_mapping.keys())),\n",
    "    flags=re.IGNORECASE|re.DOTALL)\n",
    "    \n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                            if contraction_mapping.get(match)\\\n",
    "                            else contraction_mapping.get(match.lower())\n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "    expanded_sentence = contractions_pattern.sub(expand_match, sentence)\n",
    "    return expanded_sentence\n",
    "\n",
    "expanded_corpus = [expand_contractions(sentence, CONTRACTION_MAP)\n",
    "for sentence in cleaned_corpus]\n",
    "print (expanded_corpus)\n",
    "print(\"-------------------------------------------------------\")\n",
    "print (CONTRACTION_MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is For lowercase\n",
      "the brown fox wasn't that quick and he couldn't win the race\n",
      "------------------------------------------\n",
      "This is For Uppercase\n",
      "THE BROWN FOX WASN'T THAT QUICK AND HE COULDN'T WIN THE RACE\n"
     ]
    }
   ],
   "source": [
    "print(\"This is For lowercase\")\n",
    "print (corpus[0].lower())\n",
    "print(\"------------------------------------------\")\n",
    "print(\"This is For Uppercase\")\n",
    "print (corpus[0].upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/krisna/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "def remove_stopwords(tokens):\n",
    "    stopword_list = nltk.corpus.stopwords.words('english')\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    return filtered_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['The', 'brown', 'fox', 'quick', 'could', 'win', 'race']], [['Hey', 'great', 'deal', '!'], ['I', 'bought', 'phone', '199']], [['You', 'learn', 'lot', 'book', '.'], ['Python', 'amazing␣↪language', '!']]]\n"
     ]
    }
   ],
   "source": [
    "expanded_corpus_tokens = [tokenize_text(text)for text in expanded_corpus]\n",
    "filtered_list_3 = [[remove_stopwords(tokens)for tokens in sentence_tokens]for sentence_tokens in expanded_corpus_tokens]\n",
    "print (filtered_list_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correcting Repeating Characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'schooool', 'is', 'realllllyyy', 'amaaazingggg']\n"
     ]
    }
   ],
   "source": [
    "# removing repeated characters\n",
    "sample_sentence = 'My schooool is realllllyyy amaaazingggg'\n",
    "sample_sentence_tokens = tokenize_text(sample_sentence)[0]\n",
    "print (sample_sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/krisna/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "def remove_repeated_characters(tokens):\n",
    "    repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    match_substitution = r'\\1\\2\\3'\n",
    "def replace(old_word):\n",
    "    if wordnet.synsets(old_word):\n",
    "        return old_word\n",
    "    new_word = repeat_pattern.sub(match_substitution, old_word)\n",
    "    return replace(new_word) if new_word != old_word else new_word\n",
    "        \n",
    "    correct_tokens = [replace(word) for word in tokens]\n",
    "    return correct_tokens\n",
    "print (remove_repeated_characters(sample_sentence_tokens))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens(text):\n",
    "    \"\"\"\n",
    "    Get all words from the corpus\n",
    "    \"\"\"\n",
    "    return re.findall('[a-z]+', text.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 80030), ('of', 40025), ('and', 38313), ('to', 28766), ('in', 22050), ('a', 21155), ('that', 12512), ('he', 12401), ('was', 11410), ('it', 10681)]\n"
     ]
    }
   ],
   "source": [
    "fd=open(\"big.txt\",\"+r\")\n",
    "data=fd.read()\n",
    "WORDS=tokens(data)\n",
    "WORD_COUNTS = collections.Counter(WORDS)\n",
    "# top 10 words in corpus\n",
    "print (WORD_COUNTS.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def known(words):\n",
    "    \"\"\"\n",
    "    Return the subset of words that are actually\n",
    "    in our WORD_COUNTS dictionary.\n",
    "    \"\"\"\n",
    "    return {w for w in words if w in WORD_COUNTS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edits0(word):\n",
    "    \"\"\"\n",
    "    Return all strings that are zero edits away\n",
    "    from the input word (i.e., the word itself).\n",
    "    \"\"\"\n",
    "    return {word}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edits1(word):\n",
    "    \"\"\"\n",
    "    Return all strings that are one edit away\n",
    "    from the input word.\n",
    "    \"\"\"\n",
    "    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    def splits(word):\n",
    "        \"\"\"\n",
    "        Return a list of all possible (first, rest) pairs\n",
    "        that the input word is made of.\n",
    "        \"\"\"\n",
    "    return [(word[:i], word[i:])\n",
    "            for i in range(len(word)+1)]\n",
    "    pairs = splits(word)\n",
    "    deletes= [a+b[1:]               for (a, b) in pairs if b]\n",
    "    transposes = [a+b[1]+b[0]+b[2:] for (a, b) in pairs if len(b) > 1]\n",
    "    replaces= [a+c+b[1:]            for (a, b) in pairs for c in alphabet if b]\n",
    "    inserts= [a+c+b                 for (a, b) in pairs for c in alphabet]\n",
    "    return set(deletes + transposes + replaces + inserts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edits2(word):\n",
    "    \"\"\"Return all strings that are two edits away\n",
    "    from the input word.\n",
    "    \"\"\"\n",
    "    return {e2 for e1 in edits1(word) for e2 in edits1(e1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(word):\n",
    "    \"\"\"\n",
    "    Get the best correct spelling for the input word\n",
    "    \"\"\"\n",
    "# Priority is for edit distance 0, then 1, then 2\n",
    "# else defaults to the input word itself.\n",
    "    candidates = (known(edits0(word)) or\n",
    "    known(edits1(word)) or\n",
    "    known(edits2(word)) or\n",
    "    [word])\n",
    "    return max(candidates, key=WORD_COUNTS.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_match(match):\n",
    "    \"\"\" \n",
    "    Spell-correct word in match,\n",
    "    and preserve proper upper/lower/title case.\n",
    "    \"\"\"\n",
    "    word = match.group()\n",
    "    def case_of(text):\n",
    "        \"\"\"\n",
    "        Return the case-function appropriate\n",
    "        for text: upper, lower, title, or just str.:\n",
    "        \"\"\"\n",
    "        return (str.upper if text.isupper() else\n",
    "                str.lower if text.islower() else\n",
    "                str.title if text.istitle() else\n",
    "                str)\n",
    "    return case_of(word)(correct(word.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_text_generic(text):\n",
    "    \"\"\"\n",
    "    Correct all the words within a text,\n",
    "    returning the corrected text.\n",
    "    \"\"\"\n",
    "    return re.sub('[a-zA-Z]+', correct_match, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fianlly\n"
     ]
    }
   ],
   "source": [
    "print (correct_text_generic('fianlly'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Porter Steammer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jump jump jump\n",
      "lie\n",
      "strang\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "print (ps.stem('jumping'), ps.stem('jumps'), ps.stem('jumped'))\n",
    "print (ps.stem('lying'))\n",
    "print (ps.stem('strange'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jump jump jump\n",
      "lying\n",
      "strange\n"
     ]
    }
   ],
   "source": [
    "# lancaster stemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "ls = LancasterStemmer()\n",
    "print (ls.stem('jumping'), ls.stem('jumps'), ls.stem('jumped'))\n",
    "print (ls.stem('lying'))\n",
    "print (ls.stem('strange'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jump jump jump\n",
      "ly\n",
      "strange\n"
     ]
    }
   ],
   "source": [
    "# regex stemmer\n",
    "from nltk.stem import RegexpStemmer\n",
    "rs = RegexpStemmer('ing$|s$|ed$', min=4)\n",
    "print (rs.stem('jumping'), rs.stem('jumps'), rs.stem('jumped'))\n",
    "print (rs.stem('lying'))\n",
    "print (rs.stem('strange'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supported Languages: ('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n",
      "autobahn\n",
      "spring\n"
     ]
    }
   ],
   "source": [
    "# snowball stemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "ss = SnowballStemmer(\"german\")\n",
    "print ('Supported Languages:', SnowballStemmer.languages)\n",
    "# autobahnen -> cars\n",
    "# autobahn -> car\n",
    "print (ss.stem('autobahnen'))\n",
    "# springen -> jumping\n",
    "# spring -> jump\n",
    "print (ss.stem('springen'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lematisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\n",
      "men\n"
     ]
    }
   ],
   "source": [
    "#Lemmatize nouns\n",
    "print (wnl.lemmatize('cars', 'n'))\n",
    "print (wnl.lemmatize('men', 'n'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "eat\n"
     ]
    }
   ],
   "source": [
    "#Lemmatize verbs\n",
    "print (wnl.lemmatize('running', 'v'))\n",
    "print (wnl.lemmatize('ate', 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sad\n",
      "fancy\n"
     ]
    }
   ],
   "source": [
    "#Lemmatize adjectives\n",
    "print (wnl.lemmatize('saddest', 'a'))\n",
    "print (wnl.lemmatize('fancier', 'a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ate\n",
      "fancier\n"
     ]
    }
   ],
   "source": [
    "#Inefective Lemmatization\n",
    "print (wnl.lemmatize('ate', 'n'))\n",
    "print (wnl.lemmatize('fancier', 'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNDERSTANDING TEXT AND STRUCTURE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'The brown fox is quick and he is jumping over the lazy dog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DET'), ('brown', 'ADJ'), ('fox', 'NOUN'), ('is', 'VERB'), ('quick', 'ADJ'), ('and', 'CONJ'), ('he', 'PRON'), ('is', 'VERB'), ('jumping', 'VERB'), ('over', 'ADP'), ('the', 'DET'), ('lazy', 'ADJ'), ('dog', 'NOUN')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/krisna/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/krisna/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('universal_tagset')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged_sent= nltk.pos_tag(tokens, tagset='universal')\n",
    "print (tagged_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to /home/krisna/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "#building our tagger\n",
    "#preparing the data\n",
    "nltk.download('treebank')\n",
    "from nltk.corpus import treebank\n",
    "data= treebank.tagged_sents()\n",
    "train_data=data[:3500]\n",
    "test_data=data[3500:]\n",
    "print (train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13893/1668413472.py:23: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(rt.evaluate(test_data))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24039113176493368\n",
      "[('The', 'NN'), ('brown', 'NN'), ('fox', 'NN'), ('is', 'NNS'), ('quick', 'NN'), ('and', 'NN'), ('he', 'NN'), ('is', 'NNS'), ('jumping', 'VBG'), ('over', 'NN'), ('the', 'NN'), ('lazy', 'NN'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "#regex tagger\n",
    "from nltk.tag import RegexpTagger\n",
    "#define regex tag patterns\n",
    "patterns = [\n",
    "(r'.*ing$', 'VBG'),\n",
    "(r'.*ed$', 'VBD'),\n",
    "(r'.*es$', 'VBZ'),\n",
    "(r'.*ould$', 'MD'),\n",
    "(r'.*\\'s$', 'NN$'),\n",
    "(r'.*s$', 'NNS'),\n",
    "(r'^-?[0-9]+(.[0-9]+)?$', 'CD'),\n",
    "(r'.*', 'NN')\n",
    "]\n",
    "rt=RegexpTagger(patterns)\n",
    "# gerunds\n",
    "# simple past\n",
    "# 3rd singular present\n",
    "# modals\n",
    "# possessive nouns\n",
    "# plural nouns\n",
    "# cardinal numbers\n",
    "# nouns (default) ...\n",
    "print(rt.evaluate(test_data))\n",
    "print(rt.tag(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP A/DT Lorillard/NNP spokewoman/NN)\n",
      "  said/VBD\n",
      "  ,/,\n",
      "  ``/``\n",
      "  (NP This/DT)\n",
      "  is/VBZ\n",
      "  (NP an/DT old/JJ story/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank_chunk\n",
    "data = treebank_chunk.chunked_sents()\n",
    "#print( data)\n",
    "train_data = data[:4000]\n",
    "test_data = data[4000:]\n",
    "print (train_data[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP A/DT Lorillard/NNP spokewoman/NN)\n",
      "  said/VBD\n",
      "  ,/,\n",
      "  ``/``\n",
      "  (NP This/DT)\n",
      "  is/VBZ\n",
      "  (NP an/DT old/JJ story/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "train_sent=train_data[7]\n",
    "print(train_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.chunk import RegexpParser\n",
    "from nltk.chunk.util import tree2conlltags, conlltags2tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'DT', 'B-NP'),\n",
       " ('Lorillard', 'NNP', 'I-NP'),\n",
       " ('spokewoman', 'NN', 'I-NP'),\n",
       " ('said', 'VBD', 'O'),\n",
       " (',', ',', 'O'),\n",
       " ('``', '``', 'O'),\n",
       " ('This', 'DT', 'B-NP'),\n",
       " ('is', 'VBZ', 'O'),\n",
       " ('an', 'DT', 'B-NP'),\n",
       " ('old', 'JJ', 'I-NP'),\n",
       " ('story', 'NN', 'I-NP'),\n",
       " ('.', '.', 'O')]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wtc=tree2conlltags(train_sent)\n",
    "wtc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP A/DT Lorillard/NNP spokewoman/NN)\n",
      "  said/VBD\n",
      "  ,/,\n",
      "  ``/``\n",
      "  (NP This/DT)\n",
      "  is/VBZ\n",
      "  (NP an/DT old/JJ story/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "tree=conlltags2tree(wtc)\n",
    "print (tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sh: 1: Xvfb: not found\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.system('Xvfb :1 -screen 0 1600x1200x16 &')\n",
    "os.environ['DISPLAY']=':1.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.stanford import StanfordDependencyParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13893/683659782.py:1: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  sdp = StanfordDependencyParser(path_to_jar='/home/krisna/Pratikum_NLP/PBA/stanford-parser-4.2.0/stanford-parser-full-2020-11-17/stanford-parser.jar',\n"
     ]
    }
   ],
   "source": [
    "sdp = StanfordDependencyParser(path_to_jar='/home/krisna/Pratikum_NLP/MODUL_1_NLP/stanford-parser-4.2.0/stanford-parser-full-2020-11-17/stanford-parser.jar',\n",
    "path_to_models_jar='/home/krisna/Pratikum_NLP/MODUL_1_NLP/stanford-parser-4.2.0/stanford-parser-full-2020-11-17/stanford-parser-4.2.0-models.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = list(sdp.raw_parse(sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "[item for item in result[0].triples()]\n",
    "dep_tree = [parse.tree() for parse in result][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(quick (fox The brown) is (jumping and he is (dog over the lazy)))\n"
     ]
    }
   ],
   "source": [
    "print (dep_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constituency-based parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.stanford import StanfordParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13893/329191783.py:1: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  scp = StanfordDependencyParser(path_to_jar='/home/krisna/Pratikum_NLP/PBA/stanford-parser-4.2.0/stanford-parser-full-2020-11-17/stanford-parser.jar',\n"
     ]
    }
   ],
   "source": [
    "scp = StanfordDependencyParser(path_to_jar='/home/krisna/Pratikum_NLP/PBA/stanford-parser-4.2.0/stanford-parser-full-2020-11-17/stanford-parser.jar',\n",
    "path_to_models_jar='/home/krisna/Pratikum_NLP/PBA/stanford-parser-4.2.0/stanford-parser-full-2020-11-17/stanford-parser-4.2.0-models.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = list(scp.raw_parse(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<DependencyGraph with 14 nodes>]\n"
     ]
    }
   ],
   "source": [
    "print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
